
1. Inspired from the article by [Mark Hill](https://www.sigarch.org/increasing-your-research-impact/) and then stumbling upon a recent [article](https://medium.com/@nidhibits224/multi-query-attention-speeding-ai-ad8fa1626b82) from my mentor during my first internship at TI, I realized how important it is to form hypothesis while doing research and prove it by doing back of the enevelope calculations. Lately, I have seen many people dissing the idea of doing excel-analysis. I still agree that it should not be the only analysis while publishing a paper. However, I realized it is still the foundation of doing research.
2. "The purpose of (scientific) computing is insight, not numbers" -- Richard Hamming. This quote reminded me one of his famous talks on [You and Your Research](https://www.cs.virginia.edu/~robins/YouAndYourResearch.html).
3. [Perplexity](https://huggingface.co/transformers/v3.3.1/perplexity.html) is not just confusion, it's kind of a prediction measure for transformers which can be casually defined as prob(word/context). Technically, it is the exponentiated average log-likelihood of a sequence.
4. To look for important problems, students should watch trends by looking at recent studies, talks and engaging with industry. For a good collaboration and to continue it in the future, both sides have to contribute well and more/less equally. Need to have fire in the belly to do good work. Need to communicate well, both orally and written communication. Learn to take criticism, and separate criticism of work with personal criticism. Success is a combination of internal satisfaction and external recognition. -- [Computer arch podcast](https://www.podbean.com/ew/pb-zdv2g-fd542d) with Mark Hill. Led me to another inspiring article about [Susan Eggers](https://www.computer.org/publications/tech-news/events/susan-eggers-computer-architect-pioneer-professor) who is the first woman to get the [Eckert Mauchly Award](https://awards.acm.org/eckert-mauchly).
5. Pipeline parallelism is good for training because when a GPU is done with their fraction of layers, they can start processing the next batch. Hoewever, not so great for inference unless you have multiple requests coming in. [Pipeline parallelism has lower communication](https://kipp.ly/blog/transformer-inference-arithmetic/#:~:text=A%20pipeline%20parallel,number%20of%20accelerators.) overhead compared to model parallism.
6. [Correlation does not mean causation](https://youtu.be/YAAHJm1pi1E). When we look at a linear correlation graph, we generally consider the X axis to be the independent variable and the Y axis to be the dependent variable. But this is not necesarily true. It could be a coincidence or X may cause Y or Y may cause X or both of them contribute to each other through feedback or there might be something totally different and more important which is causing the change to happen or rather a common cause affecting both X and Y. Also, if one thing happens in time before the other one, it does not mean the former caused the later one ([Post hoc propter ergo hoc fallacy example](https://xkcd.com/925/)).
7. Sharding - typically refers to spatial partitioning. Tiling - typically covers both temporal and spatial partitioning. On a more general level: Tiling is commonly used by comp arch and compiler folks (other terms - blocking etc) - tiling matmul, other compute etc. Sharding is commonly used by system architects (system SW design etc.)  - sharding database, file systems (model params, checkpoint etc). Courtesy: [Aayush](https://github.com/Aayush-Ankit).
8. Associative interference happens when brain relates a new pattern with an already known one if they have some kind of association. (read in the book [Lessons in Chemistry](https://www.goodreads.com/en/book/show/58065033). My understanding was that associative interference is the phenomenon which helps in creatively coming up with solutions for problems or re-learning a concept.
